{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f9b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lib.Metrics as Metrics\n",
    "from lib.models.IRNN_Full_Bayes import IRNN_Full_Bayes\n",
    "from lib.models.IRNN import IRNN\n",
    "from lib.train_functions import fit\n",
    "from lib.utils import *\n",
    "from lib.regional_data_builder import DataConstructor\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to evaluate a set of hyper parameters. \n",
    "# Using a class allows the __call__ function to be used for different models with different configurations\n",
    "# class is used wih bayesian optimization to find the best hyper parameters\n",
    "class Eval_Fn:\n",
    "    def __init__(self, root='', model=None, region='US', n_regions=1, n_folds = 5, test_season = 2017, gamma=28, plot=True, verbose=True, min_score=-25, batch_size=32, window_size = 28, **kwargs):\n",
    "        self.model = model  \n",
    "        self.n_folds = n_folds      # number of folda (k fold cross validation)\n",
    "        self.test_season = test_season\n",
    "        self.gamma = gamma      \n",
    "        self.plot = plot            # save plots validation set forecasts  \n",
    "        self.verbose = verbose      # print during training\n",
    "        self.min_score = min_score  # score to give hyper paremeters if they break and get -infinity\n",
    "        self.root = root            # save directory\n",
    "        self.region = region\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_regions = n_regions\n",
    "\n",
    "    def __call__(self, batch_size = 32, **kwargs):\n",
    "        tf.keras.backend.clear_session()\n",
    "        score = {}\n",
    "        plt.clf()\n",
    "\n",
    "        kwargs['n_op'] = int(kwargs['n_op'])\n",
    "\n",
    "        _data = DataConstructor(test_season=self.test_season, region = self.region, window_size=self.window_size, n_queries=kwargs['n_op']-1, gamma=28)\n",
    "        x_train, y_train, x_test, y_test, scaler = _data()\n",
    "\n",
    "        try:\n",
    "            scaler = scaler[np.newaxis, np.newaxis, :]\n",
    "        except:\n",
    "            scaler = scaler.iloc[0]\n",
    "\n",
    "        x_train = tf.cast(x_train, tf.float32)\n",
    "        y_train = tf.cast(y_train, tf.float32)\n",
    "        x_test = tf.cast(x_test, tf.float32)\n",
    "        y_test = tf.cast(y_test, tf.float32)\n",
    "\n",
    "        for fold in range(self.n_folds):\n",
    "            try:\n",
    "    \n",
    "                x_val = x_train[-(365*(fold+1)): -(365*(fold)+1)]\n",
    "                y_val = y_train[-(365*(fold+1)): -(365*(fold)+1)]\n",
    "                val_dates = _data.train_dates[-365*(fold+1): -(365*fold)-1]\n",
    "\n",
    "                x_tr = x_train[:-(365*(fold+1))]\n",
    "                y_tr = y_train[:-(365*(fold+1))]\n",
    "\n",
    "                train_dataset = tf.data.Dataset.from_tensor_slices((x_tr, y_tr))\n",
    "                train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "                _model = self.model(**kwargs)\n",
    "\n",
    "                # define loss, epochs learning rate\n",
    "                if _model.loss == 'NLL':\n",
    "                    def loss(y, p_y):\n",
    "                        return -p_y.log_prob(y)\n",
    "                if _model.loss == 'MSE':\n",
    "                    loss = tf.keras.losses.mean_squared_error\n",
    "\n",
    "                if 'epochs' in kwargs:\n",
    "                    epochs = int(kwargs['epochs'])\n",
    "                else:\n",
    "                    epochs = self.epochs\n",
    "\n",
    "                if 'lr_power' in kwargs:\n",
    "                    lr = np.power(10, kwargs['lr_power'])\n",
    "                else:\n",
    "                    lr = 1e-3\n",
    "\n",
    "                def loss_fn(y, p_y):\n",
    "                    return -p_y.log_prob(y)\n",
    "\n",
    "                _model(x_val)\n",
    "\n",
    "                pred = _model(x_test)\n",
    "                optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "                _model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "                _model.fit(train_dataset, epochs =epochs, verbose=self.verbose)\n",
    "\n",
    "                y_pred = _model.predict(x_val, 25, verbose=self.verbose)\n",
    "\n",
    "                std = (y_pred[0]+y_pred[1])[..., -self.n_regions:] * scaler - y_pred[0][..., -self.n_regions:] * scaler\n",
    "                mean = y_pred[0][..., -self.n_regions:] * scaler\n",
    "                y_te = y_val[..., -self.n_regions:]*scaler\n",
    "\n",
    "                df = {g:pd.DataFrame(columns = ['True', 'Pred', 'Std'], data = np.asarray([y_te[:, g, -1], mean[:, g, -1], std[:, g, -1]]).T) for g in [6,13,20,27]}\n",
    "\n",
    "                # get score for fold, 2 options depending on whether the forecast is a list (IRNN) or a single prediction\n",
    "                try:\n",
    "                    score[fold] = Metrics.nll(df[self.gamma])\n",
    "                except:\n",
    "                    score[fold] = np.sum(np.asarray([Metrics.nll(d) for d in df.values()]))\n",
    "\n",
    "                # can be useful to plot the validation curves to check things are working \n",
    "                if self.plot:\n",
    "                    for idx, d in enumerate(df.values()):\n",
    "                        plt.subplot(len(df.keys()), 1, idx+1)\n",
    "                        plt.plot(d.index, d['True'], color='black')\n",
    "                        plt.plot(d.index, d['Pred'], color='red')\n",
    "                        plt.fill_between(d.index, d['Pred']+d['Std'], d['Pred']-d['Std'], color='red', alpha=0.3, linewidth=0)\n",
    "            except Exception as e:\n",
    "                score[fold] = -self.min_score\n",
    "                print(e)\n",
    "\n",
    "        if self.plot:\n",
    "            if not os.path.exists(self.root):\n",
    "                os.makedirs(self.root)\n",
    "\n",
    "            figs = os.listdir(self.root)\n",
    "            nums = [-1]\n",
    "\n",
    "            for f in figs:\n",
    "                if 'fig' in f:\n",
    "                    nums.append(int(f.split('_')[1].split('.')[0]))\n",
    "\n",
    "            plt.savefig(os.path.join(self.root, 'fig_{}.pdf'.format(max(nums) + 1)))\n",
    "\n",
    "        try:\n",
    "            # NLL can be give nan values, try to prevent this breaking things\n",
    "            if np.isfinite(-sum(score.values())):\n",
    "                return -sum(score.values())\n",
    "            else:\n",
    "                return self.min_score\n",
    "        except:\n",
    "            return self.min_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Eval_Fn(model=IRNN, n_folds = 2, plot=False, verbose=False, root='Optimisation/Plots/')\n",
    "test_eval_score = eval(gamma = 28,\n",
    "                        epochs= 3,\n",
    "                        kl_power= -2.857091693154802,\n",
    "                        lr_power= -3.7364141761644545,\n",
    "                        n_op= 93,\n",
    "                        op_scale_pwr= -0.27139484469983133,\n",
    "                        p_scale_pwr= -1.827071638197097,\n",
    "                        q_scale_pwr= -1.2461978663286395,\n",
    "                        rnn_units= 108\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_iter =250\n",
    "model = IRNN\n",
    "gamma = 28\n",
    "root = 'Optimisation/IRNN/'\n",
    "\n",
    "n_folds = 4 # increase this to improve rubustness, will get slower\n",
    "eval = Eval_Fn(model=IRNN, \n",
    "               root = root, gamma=gamma, plot=False, n_folds=n_folds, verbose=False)\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=eval,\n",
    "    pbounds=model.pbounds,\n",
    "    random_state=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "optimizer = load_steps(root, optimizer)\n",
    "\n",
    "for _ in range(100):\n",
    "    optimizer.maximize(\n",
    "        init_points=10,\n",
    "        n_iter=10)\n",
    "\n",
    "    save_steps(root, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
